{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-picnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-voltage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "worst-mother",
   "metadata": {},
   "source": [
    "## Key Concepts in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-alexandria",
   "metadata": {},
   "source": [
    "1) frameworks<br>\n",
    "2) libraries<br>\n",
    "3) rules-based NLP<br>\n",
    "4) machine learning-based NLP<br>\n",
    "5) tokenization<br>\n",
    "6) chunks<br>\n",
    "7) noun extraction<br>\n",
    "8) part-of-speech identification<br>\n",
    "9) entity identification<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-extent",
   "metadata": {},
   "source": [
    "## What are Frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-stereo",
   "metadata": {},
   "source": [
    "In order to engage in NLP, a researcher must first decide upon the framework they wish to use. **Framework** is a word that describes the software used by the researcher to engage in a specific task. A good way to think about a framework in Pythonic terms is as a **library**, or packaged set of usable classes and functions to perform complex tasks easily. Deciding which framework to use depends on a few variables.\n",
    "\n",
    "First, not all frameworks support all languages and not all frameworks support the same languages equally.\n",
    "\n",
    "Second, certain frameworks perform certain tasks better than others. While all frameworks will tokenize equally well (usually), the way in which some tasks, such as finding the root of words via lemmatization (spaCy) vs. stemming (Stanza) will vary. Decision on a framework for this purpose typically lies in the realm of computational linguistics or distance reading for the purpose of finding how a word (or words) appear in texts in all forms (conjugated and declined).\n",
    "\n",
    "A common third thing to consider is the way in which the framework performs NLP. There are essentially two methods for performing NLP: rules-based and machine learning-based. **Rules-based** NLP is the process by which the frameworks has a predetermined set of rules for how to handle specific tasks. In order to find entities in a text, for example, a rules-based method will contain a dictionary of all types of entities or it may contain a RegEx formula for identifying patterns that match an entity.\n",
    "\n",
    "Most frameworks today are moving away from a rules-based approach to NLP in favor of a machine learning-based approach. **Machine learning-based** NLP is the process by which developers use statistics to teach a computer system (known as a model) to perform a task based on past experiences (known as training). We will be speaking much more about machine learning-based NLP later in a later notebook as spaCy, the chief subject of this notebook, is a machine learning-based Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-cleveland",
   "metadata": {},
   "source": [
    "## What is spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-federation",
   "metadata": {},
   "source": [
    "The spaCy (spelled correctly) library is a robust machine learning NLP library developed by Explosion AI, a Berlin based team of computer scientists and computational linguists. It supports a wide variety of European languages out-of-the-box with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. SpaCy is also capable of easily improving or training from scratch custom models on domain-specific texts.\n",
    "\n",
    "In this notebook, we will go through the steps for installing spaCy, downloading a pretrained language model, and performing the essential tasks of NLP.\n",
    "\n",
    "In order to download and install spaCy and the model, outside of a notebook, please watch the video below and follow the necessary steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contrary-launch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yqruv_QQctI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yqruv_QQctI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-focus",
   "metadata": {},
   "source": [
    "Because we are working within a notebook, we can execute terminal commands in Juptyer cells with a \"!\" In the cells below, we will import the spaCy library and then download the English small model. We will then create an object called \"nlp\" and load that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "synthetic-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-25 19:19:03.663484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-ticket",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-independence",
   "metadata": {},
   "source": [
    "A common essential task of NLP is known as tokenization. We looked at tokenization briefly in the last notebook in which we wanted to break a text into individual components. This is one form of tokenization known as word tokenization. There are, however, many other forms, such as sentence tokenization. **Sentence tokenization** is precisely the same as word tokenization, except instead of breaking a text up into individual word and punctuation components, we break a text up into individual sentences.\n",
    "\n",
    "If you are familiar with Python, you may be familiar with the built-in split() function which allows for a programmer to split a text by whitespace (default) or by passing an argument of a string to define where to split a text, i.e. split(\".\"). A common practice (without NLP frameworks) is to split a text into sentences by simply using the split function, but this is ill-advised. Let us consider the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metallic-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin J', ' Thompson is known for his writing skills', ' He is also good at programming', '']\n"
     ]
    }
   ],
   "source": [
    "#Now, let's try and use the split function to split the text object based on punctuation.\n",
    "text = \"Martin J. Thompson is known for his writing skills. He is also good at programming.\"\n",
    "new = text.split(\".\")\n",
    "print (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-defensive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "functional-disaster",
   "metadata": {},
   "source": [
    "While we successfully were able to split the two sentences, we had the unfortunate result of splitting at Martin J. The reason for this may be obvious. In English, it is common convention to indicate abbreviation with the same punctuation mark used to indicate the end of a sentence. The reason for this extends to the early middle ages when Irish monks began to introduce punctuation and spacing to better read Latin (a story for another day).\n",
    "\n",
    "The very thing that makes texts easier to read, however, greatly hinders our ability to easily split sentences. For this reason, another method is needed. This is where sentence tokenization comes into play. In order to see how sentence tokenization differs, let's begin with our first spaCy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eleven-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cubic-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we need to load an NLP model object.\n",
    "To do this, we use the spacy.load() function.\n",
    "This will take one argument, the model one wishes to load.\n",
    "We will use the small English model.\n",
    "'''\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honey-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "With the nlp object created, we can use it to to parse a text.\n",
    "To do this, we create a doc object.\n",
    "This object will contain a lot of data on the text.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norman-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson is known for his writing skills. He is also good at programming.\n"
     ]
    }
   ],
   "source": [
    "#try printing the object:\n",
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stone-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson is known for his writing skills.\n",
      "He is also good at programming.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "While this looks identical to the \"text\" string above, it is quite different.\n",
    "To demonstrate this, let us use the sentence tokenizer.\n",
    "'''\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-factor",
   "metadata": {},
   "source": [
    "Notice now that we have used the spaCy sentence tokenizer to generate a desired output: a text correctly broken into sentences. This simple demonstration reveals why using an NLP framework for perfoming even a basic task is not only easier, but essential. For a larger explanation of this process, please watch the video below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collaborative-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ytAyCO-n8tY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ytAyCO-n8tY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-venezuela",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-raise",
   "metadata": {},
   "source": [
    "Another essential task of NLP, and the chief subject of this series, is named entity recognition (NER). I spoke about NER in the last notebook. Here, I'd like to demonstrate how to perform basic NER via spaCy. Again, we will iterate over the doc object as we did above, but instead of iterating over doc.sents, we will iterate over doc.ents. For our purposes right now, I simply want to print off each entity's text (the string itself) and its corresponding label (note the _ after label). I will be explaining this process in much greater detail in the next two notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "missing-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "liquid-tourism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lxHNsXudkrY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lxHNsXudkrY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-sailing",
   "metadata": {},
   "source": [
    "## Part-of-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-christianity",
   "metadata": {},
   "source": [
    "In the field of computational linguistics, understanding parts-of-speech is essential. SpaCy offers an easy way to parse a text and identify its parts of speech. Below, we will iterate across each token (word or punctuation) in the text and identify its part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interim-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin PROPN\n",
      "J. PROPN\n",
      "Thompson PROPN\n",
      "is AUX\n",
      "known VERB\n",
      "for ADP\n",
      "his PRON\n",
      "writing NOUN\n",
      "skills NOUN\n",
      ". PUNCT\n",
      "He PRON\n",
      "is AUX\n",
      "also ADV\n",
      "good ADJ\n",
      "at ADP\n",
      "programming NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-bernard",
   "metadata": {},
   "source": [
    "Here, we can see two vital pieces of information: the string and the corresponding part-of-speech (pos). For a complete list of the pos labels, see the spaCy documentation (https://spacy.io/api/annotation#pos-tagging). Most of these, however, should be apparent, i.e. PROPN is proper noun, AUX is an auxillary verb, ADJ, is adjective, etc. For more on this process, please see the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nv0pksknFxY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nv0pksknFxY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-thesis",
   "metadata": {},
   "source": [
    "## Extracting Nouns and Noun Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-wells",
   "metadata": {},
   "source": [
    "Often times when working with a text, we need to extract nouns and noun chunks. There are a few different ways that we can do this via spaCy. To extract nouns, we can use the doc.noun_chunks attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "female-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson\n",
      "his writing skills\n",
      "He\n",
      "programming\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-assault",
   "metadata": {},
   "source": [
    "Note that we get a list of all nouns and noun chunks, i.e. \"He\" and \"programming\" being nouns and \"Martin J. Thompson\" and \"his writing skills\" being noun chunks. For more on this, please see the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "registered-candidate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aNKt1gKK8Lo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aNKt1gKK8Lo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-collaboration",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-tournament",
   "metadata": {},
   "source": [
    "The final item I'd like to explore in this notebook is lemmatization. **Lemmatization** is an essential component in most NLP frameworks, though some libraries perform this concept differently. While libraries, such as Stanza will find word stems, spaCy will find word lemmas. They are technically a little different, but both seek to reduce all words to their roots. To find lemmas via spaCy, we use the same process as we did for finding a word's part of speech, via iterating over the tokens in the doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "solved-bracelet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Martin\n",
      "J. J.\n",
      "Thompson Thompson\n",
      "is be\n",
      "known know\n",
      "for for\n",
      "his his\n",
      "writing writing\n",
      "skills skill\n",
      ". .\n",
      "He he\n",
      "is be\n",
      "also also\n",
      "good good\n",
      "at at\n",
      "programming programming\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-marketplace",
   "metadata": {},
   "source": [
    "Note that we see most words remain the same, but notice particularly \"is\" being identified as \"be\" and \"known\" becomes \"know\". These are the respective lemmas for these verbs. Also notice the same effect on nouns, such as \"skills\", a plural, being reduced to \"skill\", the singular form. For more on this, see the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comparable-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YztOLsJkC3A\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div align=\"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YztOLsJkC3A\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-valentine",
   "metadata": {},
   "source": [
    "## Loading Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "missing-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/alice.txt\", \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "invalid-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = corpus.split(\"CHAPTER\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "optical-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_one = chapters[0].split(\"\\n\\n\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "oriental-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = chap_one.split(\"\\n\\n\")\n",
    "para_one = paragraphs[0].replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceramic-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(para_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "civil-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do:  once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "significant-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell fill in the gaps. Try to print off a list of entities and their labels.\n",
    "\n",
    "for ent in doc.ents:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-combining",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atmospheric-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indian-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "\n",
    "def write_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "killing-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gernating rules\n",
    "def generate_ruler(patterns, name):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    ruler.to_disk(f\"models/{name}_ent_ruler/entity_ruler/patterns.jsonl\") \n",
    "    nlp.to_disk(f\"models/{name}_ent_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fundamental-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(file, type):\n",
    "    data = load_data(file)\n",
    "    patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\n",
    "                    \"label\": type,\n",
    "                    \"pattern\": item\n",
    "                    }\n",
    "        patterns.append(pattern)\n",
    "    return (patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ent_ruler(ruler, corpus):\n",
    "    nlp = spacy.load(ruler)\n",
    "    with open (corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read()\n",
    "    with open (\"temp/results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        doc = nlp(corpus)\n",
    "        for ent in doc.ents:\n",
    "            f.write(f\"{ent.text}, {ent.label_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "brilliant-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(corpus, ent_ruler_model, output_file, prodigy=False):\n",
    "    nlp=spacy.load(ent_ruler_model)\n",
    "    TRAIN_DATA = []\n",
    "    with open (corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        segments = data.split(\"\\n\")\n",
    "        for segment in segments:\n",
    "            segment = segment.strip()\n",
    "            doc = nlp(segment)\n",
    "            entities = []\n",
    "            for ent in doc.ents:\n",
    "                if prodigy==True:\n",
    "                    entities.append({\"start\":ent.start_char, \"end\": ent.end_char,  \"label\": ent.label_, \"text\": ent.text})\n",
    "                    pass\n",
    "                else:\n",
    "                    entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "            if len(entities) > 0:\n",
    "                if prodigy==True:\n",
    "                    TRAIN_DATA.append({\"text\": segment, \"spans\": entities})\n",
    "                else:\n",
    "                    TRAIN_DATA.append([segment, {\"entities\": entities}])\n",
    "    print (len(TRAIN_DATA))\n",
    "    with open (output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(TRAIN_DATA, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accessible-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_patterns = create_training_data(\"latin_data/all_names_declined.json\", \"PERSON\")\n",
    "groups_patterns = create_training_data(\"latin_data/groups_declined.json\", \"GROUP\")\n",
    "places_patterns = create_training_data(\"latin_data/places_declined.json\", \"LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "steady-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patterns = person_patterns+groups_patterns+places_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cardiac-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ruler(all_patterns, \"latin_loc_per_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "english-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ent_ruler(\"models/latin_loc_per_group_ent_ruler\", \"latin_data/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "surprising-multimedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    }
   ],
   "source": [
    "create_training_set(\"latin_data/corpus.txt\", \"models/latin_loc_per_group_ent_ruler\", \"training_data/training_set_spacy.json\", prodigy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sound-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eleven-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = load_data(\"training_data/training_set_spacy.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "choice-buffalo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[1] Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt. Gallos ab Aquitanis Garumna flumen, a Belgis Matrona et Sequana dividit. Horum omnium fortissimi sunt Belgae, propterea quod a cultu atque humanitate provinciae longissime absunt, minimeque ad eos mercatores saepe commeant atque ea quae ad effeminandos animos pertinent important, proximique sunt Germanis, qui trans Rhenum incolunt, quibuscum continenter bellum gerunt. Qua de causa Helvetii quoque reliquos Gallos virtute praecedunt, quod fere cotidianis proeliis cum Germanis contendunt, cum aut suis finibus eos prohibent aut ipsi in eorum finibus bellum gerunt. Eorum una, pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones. Belgae ab extremis Galliae finibus oriuntur, pertinent ad inferiorem partem fluminis Rheni, spectant in septentrionem et orientem solem. Aquitania a Garumna flumine ad Pyrenaeos montes et eam partem Oceani quae est ad Hispaniam pertinet; spectat inter occasum solis et septentriones.', {'entities': [[4, 10, 'LOCATION'], [65, 71, 'GROUP'], [79, 87, 'GROUP'], [116, 122, 'GROUP'], [131, 136, 'GROUP'], [207, 213, 'GROUP'], [217, 226, 'GROUP'], [245, 251, 'GROUP'], [309, 315, 'LOCATION'], [591, 599, 'GROUP'], [616, 622, 'GROUP'], [796, 802, 'GROUP'], [848, 855, 'LOCATION'], [885, 891, 'LOCATION'], [901, 909, 'GROUP'], [929, 937, 'GROUP'], [941, 950, 'GROUP'], [991, 997, 'GROUP'], [1128, 1137, 'LOCATION'], [1190, 1196, 'LOCATION'], [1209, 1218, 'LOCATION']]}]\n"
     ]
    }
   ],
   "source": [
    "print (all_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "resistant-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = all_docs[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "administrative-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_docs = all_docs[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reverse-diving",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "charming-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 609.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_db = DocBin()\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.blank(\"en\")\n",
    "for text, annot in tqdm(train_docs):\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            pass\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    train_db.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "going-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 188/188 [00:00<00:00, 648.33it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_db = DocBin()\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.blank(\"en\")\n",
    "for text, annot in tqdm(valid_docs):\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            pass\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    train_db.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-factory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "composed-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db.to_disk(\"./training_data/train_hs.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "stone-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_db.to_disk(\"./training_data/valid_hs.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "completed-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-01 12:23:41.906175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"latin_data/livy_01.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-northern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

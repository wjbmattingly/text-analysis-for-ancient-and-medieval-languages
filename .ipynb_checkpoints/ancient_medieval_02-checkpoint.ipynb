{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "illegal-township",
   "metadata": {},
   "source": [
    "# Text Analysis with Ancient and Medieval Languages<br><br>Day 02:<br>The Problems of Under-Resourced Languages (and general solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-shadow",
   "metadata": {},
   "source": [
    "<center>Dr. William Mattingly<br>\n",
    "TAP Institute with JSTOR</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transsexual-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fasttext) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fasttext) (54.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fasttext) (1.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-exemption",
   "metadata": {},
   "source": [
    "## The Problems of NLP in Under-Resourced Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-tender",
   "metadata": {},
   "source": [
    "For those of us working with under-resourced languages, such as medieval and ancient languages, we often have some shared problems. These problems often fit into two categories: lack of textual data and lack of structured training data. Both of these are needed to perform various NLP tasks. In this notebook, I address these two categories and provide conceptual solutions to these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-vacation",
   "metadata": {},
   "source": [
    "## Getting the Data from PDF to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-piano",
   "metadata": {},
   "source": [
    "In order to create an NLP model for any language, you need textual data. Certain NLP tasks can be done with unstructured raw text, but even getting that can be a challenge. Though you may not have raw text, you may have many pdfs or scans of texts laying around. In this section, I will provide some code and some steps for converting image data into raw text via Python. To do this, we will engage in OCR, or Optical Character Recognition. OCR allows us to read an image (which is just a collection of numbers) and convert those numbers that represent pixels into meaningful text.\n",
    "\n",
    "To demonstrate this, we will be working with the following image, which is a scan from a critical edition of Alcuin's letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-ranch",
   "metadata": {},
   "source": [
    "<img src=\"data/sample_mgh.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-nudist",
   "metadata": {},
   "source": [
    "The text is in Latin and contains several common problems when trying to convert a critical edition into raw text. First, it has marginalia on the left and right. On the left hand side, the marginalia detail scriptural quotations or allusions. On the right hand side, we have the lines denoted. Both of these throw off OCR results.\n",
    "\n",
    "Second, we have header data, such as pagination and edition markings.\n",
    "\n",
    "Third, we have footer data, specifically the critical apparatus and the footnotes.\n",
    "\n",
    "Were I interested in producing a clean OCR of this text, I need a way to remove all that data automatically. Because these are scans, the marginalia will be off by as much as 20-50 pixels each image. Because footnotes vary in quantity, each page will have footer data in different locations. These problems prevent me from writing a set of rules to automate the removal of these things from the image. Fortunately, I can turn to Python's computer vision library OpenCV.\n",
    "\n",
    "Through the code below, I can manipulate this image, identify structure, and then convert the entire document into raw text via PyTesseract which acts as a wrapper between Python and Tesseract, a collection of OCR models trained by Google.\n",
    "\n",
    "Finally, I can do some post-processing cleaning to the text to get a text that is fairly accurate (with some few typographical errors). I have something that is good enough for machine learning purposes to start working with textual data. Furthermore, these scripts would allow me to automate this process and convert many critical editions from this same publisher into good quality OCR in a matter of hours.\n",
    "\n",
    "In other words, in order to solve the problem of no textual data, you need to become familiar with automating OCR via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animated-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source for this section is another TAP Institute Course by Hannah Jacobs => https://hub.binder.constellate.org/user/hlj24-tapi_2021_ocr-ncsnihj6/notebooks/01-WhatIsOCR.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorrect-champion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Install tesseract on Binder.\n",
    "# The exclamation runs the command as a terminal command.\n",
    "# This may take 1-2 minutes.\n",
    "# Source: Nathan Kelber & JStor Labs Constellate team.\n",
    "!conda install -c conda-forge -y tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dependent-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata\n",
    "!mv eng.traineddata /srv/conda/envs/notebook/share/tessdata/eng.traineddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-species",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incomplete-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pytesseract library, which will run the OCR process.\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "objective-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread(\"data/sample_mgh.JPG\")\n",
    "base_image = image.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "saving-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_footnote_line(image, base_image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernal = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 10))\n",
    "    dilate = cv2.dilate(thresh, kernal, iterations=1)\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[1])\n",
    "    main_line = []\n",
    "    for c in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if h < 25 and w > 250:\n",
    "            roi = base_image[y:y+h, x:x+w]\n",
    "#             cv2.rectangle(image, (x,y), (x+w, y+h), (36, 255, 12), 2)\n",
    "            main_line.append([x,y,w,h])\n",
    "    cv2.imwrite(\"data/sample_boxes.png\", image)\n",
    "    return (main_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "banned-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_line = find_footnote_line(image, base_image)\n",
    "x,y,w,h = main_line[0]\n",
    "new = base_image[25:y, 0:-25]\n",
    "cv2.imwrite(\"data/extraction.png\", new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "textile-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"data/final.jpg\")\n",
    "def find_body(image, base_image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernal = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 50))\n",
    "    dilate = cv2.dilate(thresh, kernal, iterations=1)\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[1])\n",
    "    main_line = []\n",
    "    for c in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if h > 200 and w > 250:\n",
    "            roi = base_image[y:y+h, x:x+w]\n",
    "#             cv2.rectangle(image, (x,y), (x+w, y+h), (36, 255, 12), 2)\n",
    "            main_line.append([x,y,w,h])\n",
    "    cv2.imwrite(\"data/body_text.png\", image)\n",
    "    return (roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handled-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = find_body(new, base_image)\n",
    "cv2.imwrite(\"data/final.jpg\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atomic-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paras(image, base_image):\n",
    "    base_image = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernal = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
    "    dilate = cv2.dilate(thresh, kernal, iterations=10)\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[1])\n",
    "    for c in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if h > 200 and w > 600:\n",
    "            roi = base_image[y:y+h, x:x+w]\n",
    "            cv2.rectangle(image, (x,y), (x+w, y+h), (36, 255, 12), 2)\n",
    "    return (roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "magnetic-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last = find_paras(final, base_image)\n",
    "cv2.imwrite(\"data/last.jpg\", last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collectible-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DILECTISSIMO* AMICO TOTIUS PROSPERITATIS PRAESENTIS ET AETERNAE\n",
      "BEATITUDINIS PERPETUAM SALUTEM.\n",
      "\n",
      "Magna mihi laetitia est de bona voluntate vestra, quam audivi a fratre nostro\n",
      "Benedicto! in vobis esse. Opto atque Deum deprecor, ut citius cum omni convenien-\n",
      "tia perficiatur. Seriptum est enim: 'Ne tardes converti ad dominum Deum; quia\n",
      "nescis, quid ventura pariat dies, Erue te de harum carcere tribulationum, quae in\n",
      "hoe mundo fidelium animos torquere solent\"; sicut scriptum est: *Multae tribulationes\n",
      "iustorum; ut, quod sequitur, tibi evenire merearis: 'Sed de his omnibus liberavit eos\n",
      "Dominus. Et cave diligentissime, ne qua te, aratrum Domini tenentem, iniustitia\n",
      "retro revocet. Nemo miles sarcinis alienis onustus ad bella bene procedit, nisi armis\n",
      "tantummodo victrieibus, vel ad defensionem sui vel ad laesionem adversarii.\n",
      "\n",
      "Omnia quae vobis demandare necessaria videbantur mihi fidelissimo fratri Bene-\n",
      "dieto dixi: loca, adiutorium et animi constantiam.\n",
      "\n",
      "Sed scire debes, quod in omni loco, ubi hominum conversatio est plurimorum,\n",
      "utrumque et boni et mali inveniuntur. Sed sapiens animus utrorumque utatur magi-\n",
      "sterio; id est, ut malorum caveat malitiam, et bonorum sequatur iustitiam. Mens\n",
      "regalis? quae homini data est, discernere debet, quae sint cavenda et quae sint se-\n",
      "quenda; nec multum de loco diffidere vel etiam confidere. Quia, si locus adiuvare\n",
      "potuisset, numquam angelus de caelo cecidisset vel homo in paradyso positus pec-\n",
      "casset. Sed regnum Dei, ut ipsa veritas ait, intra nosmetipsos quaerendum est. Et\n",
      "psalmista: \"Timete Dominum omnes sancti eius, quia nihil deest timentibus eum'.\n",
      "Timor Domini peccare vetat, dum homo ubique Dei sibi praesentiam agnoscit et timet.\n",
      "Quem* qui conscium habet cogitationum verborum vel operum suorum, hune habi-\n",
      "turus est et iudicem. Nec eum quicquam effugit nostri nec aliquid iniudicatum de-\n",
      "miserit; quia, sicut dietum est, unicuique reddet secundum opera sua, Dum tempus\n",
      "habemus, operemur bonum, quia post mortem non est tempus operandi, sed tempus\n",
      "mercedem recipiendi. Haee cogitans, carissime fili, tui ipsius curam habeto , memor,\n",
      "de quantis te liberavit Deus periculis. Illum ama et ad eius misericordiam conver-\n",
      "tere, ut deleantur delicta tua et merearis locum refrigerii lucis et pacis recipere cum\n",
      "sanctis Dei. Meique memor cum Dei servientibus pro teque intercedentibus valeas\n",
      "perpetua prosperitate, dulcissime amice.\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "ocr_result = pytesseract.image_to_string(last, lang=\"lat\")\n",
    "print (ocr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "typical-insider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sections = ocr_result.split(\"\\n\\n\")\n",
    "print (len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "organizational-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DILECTISSIMO AMICO TOTIUS PROSPERITATIS PRAESENTIS ET AETERNAE BEATITUDINIS PERPETUAM SALUTEM.\n",
      "\n",
      "Magna mihi laetitia est de bona voluntate vestra, quam audivi a fratre nostro Benedicto! in vobis esse. Opto atque Deum deprecor, ut citius cum omni convenientia perficiatur. Seriptum est enim: 'Ne tardes converti ad dominum Deum; quia nescis, quid ventura pariat dies, Erue te de harum carcere tribulationum, quae in hoe mundo fidelium animos torquere solent'; sicut scriptum est: Multae tribulationes iustorum; ut, quod sequitur, tibi evenire merearis: 'Sed de his omnibus liberavit eos Dominus. Et cave diligentissime, ne qua te, aratrum Domini tenentem, iniustitia retro revocet. Nemo miles sarcinis alienis onustus ad bella bene procedit, nisi armis tantummodo victrieibus, vel ad defensionem sui vel ad laesionem adversarii.\n",
      "\n",
      "Omnia quae vobis demandare necessaria videbantur mihi fidelissimo fratri Benedieto dixi: loca, adiutorium et animi constantiam.\n",
      "\n",
      "Sed scire debes, quod in omni loco, ubi hominum conversatio est plurimorum, utrumque et boni et mali inveniuntur. Sed sapiens animus utrorumque utatur magisterio; id est, ut malorum caveat malitiam, et bonorum sequatur iustitiam. Mens regalis? quae homini data est, discernere debet, quae sint cavenda et quae sint sequenda; nec multum de loco diffidere vel etiam confidere. Quia, si locus adiuvare potuisset, numquam angelus de caelo cecidisset vel homo in paradyso positus peccasset. Sed regnum Dei, ut ipsa veritas ait, intra nosmetipsos quaerendum est. Et psalmista: 'Timete Dominum omnes sancti eius, quia nihil deest timentibus eum'. Timor Domini peccare vetat, dum homo ubique Dei sibi praesentiam agnoscit et timet. Quem qui conscium habet cogitationum verborum vel operum suorum, hune habiturus est et iudicem. Nec eum quicquam effugit nostri nec aliquid iniudicatum demiserit; quia, sicut dietum est, unicuique reddet secundum opera sua, Dum tempus habemus, operemur bonum, quia post mortem non est tempus operandi, sed tempus mercedem recipiendi. Haee cogitans, carissime fili, tui ipsius curam habeto, memor, de quantis te liberavit Deus periculis. Illum ama et ad eius misericordiam convertere, ut deleantur delicta tua et merearis locum refrigerii lucis et pacis recipere cum sanctis Dei. Meique memor cum Dei servientibus pro teque intercedentibus valeas perpetua prosperitate, dulcissime amice. \f",
      "\n"
     ]
    }
   ],
   "source": [
    "final_sections = []\n",
    "for sec in sections:\n",
    "    sec = sec.replace(\"-\\n\", \"\")\n",
    "    sec = sec.replace(\"\\n\", \" \")\n",
    "    sec = sec.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" ;\", \";\").replace(\"*\", \" \").replace(\"\\\"\", \"\\'\")\n",
    "    while \"  \" in sec:\n",
    "        sec = sec.replace(\"  \", \" \")\n",
    "    final_sections.append(sec)\n",
    "cleaned_text = \"\\n\\n\".join(final_sections)\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-stage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-combat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "connected-spare",
   "metadata": {},
   "source": [
    "## Introduction to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-guitar",
   "metadata": {},
   "source": [
    "Word vectors, or word embeddings, take these one dimensional bag of words and gives them multidimensional meaning by representing them in higher dimensional space, noted above. This is achieved through machine learning and can be easily achieved via Python libraries, such as Gensim or FastText, which we will explore more closely later in this notebook\n",
    "\n",
    "The goal of word vectors is to achieve numerical understanding of language so that a computer can perform more complex tasks on that corpus. Let’s consider the example above. How do we get a computer to understand 2 and 6 are synonyms or mean something similar? One option you might be thinking is to simply give the computer a synonym dictionary. It can look up synonyms and then know what words mean. This approach, on the surface, makes perfect sense, but let’s explore that option and see why it cannot possibly work.\n",
    "\n",
    "For the example below, we will be using the Python library PyDictionary which allows us to look up definitions and synonyms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rural-burner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDictionary in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from PyDictionary) (0.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from PyDictionary) (2.25.1)\n",
      "Requirement already satisfied: click in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from PyDictionary) (7.1.2)\n",
      "Requirement already satisfied: goslate in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from PyDictionary) (1.5.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bs4->PyDictionary) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->bs4->PyDictionary) (2.2)\n",
      "Requirement already satisfied: futures in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from goslate->PyDictionary) (3.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->PyDictionary) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->PyDictionary) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->PyDictionary) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->PyDictionary) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "criminal-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom: ['Felis domesticus', 'tomcat', 'domestic cat', 'gib', 'house cat']\n",
      "\n",
      "loves: ['amorousness', 'caring', 'lovingness', 'agape', 'adoration']\n",
      "\n",
      "to: ['digitizer', 'data converter', 'digitiser', 'analog-digital converter']\n",
      "\n",
      "eat: ['consume', 'garbage down', 'eat up', 'gluttonize', 'take in']\n",
      "\n",
      "chocolate: ['drinking chocolate', 'drink', 'drinkable', 'potable', 'beverage']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "text = \"Tom loves to eat chocolate\"\n",
    "\n",
    "words = text.split()\n",
    "for word in words:\n",
    "        syns = dictionary.synonym(word)\n",
    "        print (f\"{word}: {syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-perry",
   "metadata": {},
   "source": [
    "Even with the simple sentence, the results are comically bad. Why? The reason is because synonym substitution, a common method of data augmentation, does not take into account syntactical differences of synonyms. I do not believe anyone would think “Felis domesticus”, the Latin name of the common house cat, would be an adequate substitution for the name Tom. Nor is “garbage down” a really proper synonym for eat.\n",
    "\n",
    "Perhaps, then we could use synonyms to find words that have cross-terms, or terms that appear in both synonym sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "diagnostic-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like: ['love', 'prefer', 'enjoy', 'cotton', 'care for']\n",
      "\n",
      "love: ['amorousness', 'caring', 'lovingness', 'agape', 'adoration']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "words  = [\"like\", \"love\"]\n",
    "for word in words:\n",
    "    syns = dictionary.synonym(word)\n",
    "    print (f\"{word}: {syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-thanks",
   "metadata": {},
   "source": [
    "This, as we can see, has some potential to work, but again it is not entirely reliable and to work with such a list would be computationally expensive. For both of these reasons, word vectors are prefered. The reason? Because they are formed by the computer on corpora for a specific task. Further, they are numerical in nature (not a dictionary of words), meaning the computer can process them more quickly.\n",
    "\n",
    "Word vectors have a preset number of dimensions. These dimensions are honed via machine learned. Models take into account word frequency alongside words across a corpus and the appearance of other words in similar contexts. This allows for the the computer to determin the syntactical similarity of words numerically. It then needs to represent these relationships numerically. It does this through the vector, or a matrix of matrices. To represent these more concisely, models flatten a matrix to a float (decimal number). The number of dimensions represent the number of floats in the matrix.\n",
    "\n",
    "Below is a pretrained model’s output of word vectors for Holocaust documents. This is how the word “know” looks in vectors:\n",
    "\n",
    "know -0.19911548 -0.27387282 0.04241912 -0.58703226 0.16149549 -0.08585547 -0.10403373 -0.112367705 -0.28902963 -0.42949626 0.051096343 -0.04708015 -0.051914077 -0.010533272 -0.23334776 0.031974062 -0.015784053 -0.21945408 0.07359381 0.04936823 -0.15373217 -0.18460844 -0.055799782 -0.057939123 0.14816307 -0.46049833 0.16128318 0.190906 -0.29180774 -0.08877125 0.23563664 -0.036557104 -0.23812544 0.21938106 -0.2781296 0.5112853 0.049084224 0.14876273 0.20611146 -0.04535578 -0.35051352 -0.26381743 0.20824358 0.29732847 -0.013382204 -0.19970295 -0.34890386 -0.16214448 -0.23497184 0.1656344 0.15815939 0.012848561 -0.22887675 -0.21618247 0.13367777 0.1028471 0.25068823 -0.13625076 -0.11771541 0.4857257 0.102198474 0.06380113 -0.22328818 -0.05281015 0.0059655504 0.095453635 0.39693353 -0.066147 -0.1920163 0.5153346 0.24972811 -0.0076305643 -0.05530072 -0.24668717 -0.074051596 0.29288396 -0.0849124 0.37786478 0.2398532 -0.10374063 0.5445305 -0.41955113 0.39866814 -0.23992492 -0.15373677 0.34488577 -0.07166888 -0.48001364 0.0660652 0.061260436 0.32197484 -0.12741785 0.024006622 -0.07915035 -0.04467735 -0.2387938 -0.07527494 0.07079664 0.074456714 0.17877163 -0.002122373 -0.16164272 0.12381973 -0.5908519 0.5827627 -0.38076186 0.095964395 0.020342976 -0.5244792 0.24467848 -0.12481717 0.2869162 -0.34473857 -0.19579992 -0.18069582 0.015281798 -0.18330036 -0.08794056 0.015334953 -0.5609912 0.17393902 0.04283724 -0.07696586 0.2040299 0.34686008 0.31219167 0.14669564 -0.26249585 -0.42771882 0.5381632 -0.123247474 -0.29142144 -0.29963812 -0.32800657 -0.10684048 -0.08594837 0.19670585 0.13474767 0.18349588 -0.4734125 0.15554792 -0.21062694 -0.14191462 -0.12800062 0.2053445 -0.05258381 0.10878109 0.56381494 0.22724482 -0.17778987 -0.061046753 0.10789692 -0.015310492 0.16563527 -0.31812978 -0.1478078 0.4323269 -0.2543924 -0.25956103 0.38653126 0.5080214 -0.18796602 -0.10318089 0.023921987 -0.14618908 0.22923793 0.37690258 0.13323267 -0.34325415 -0.048353776 -0.30283198 -0.2839813 -0.2627738 -0.07422618 -0.31940162 0.38072023 0.56700015 -0.023362642 -0.3786432 0.084006436 0.0729958 0.09483505 -0.2665334 0.12699558 -0.37927982 -0.39073908 0.0063185897 -0.34464878 -0.24011964 0.09303968 -0.15488827 -0.018486138 0.3560308 -0.26005003 0.089302294 0.116130605 0.07684872 -0.085253105 -0.28178927 -0.17346472 -0.20008522 0.004347025 0.34192443 0.017453942 0.06926512 -0.15926014 -0.018554512 0.18478563 -0.040194467 0.38450953 0.4104423 -0.016453728 0.013374495 -0.011256633 0.09106963 0.20074937 0.17310189 -0.12467103 0.16330549 -0.0009963055 0.12181527 -0.05295286 -0.0059491103 -0.04697837 0.38616535 -0.21074814 -0.32234505 0.47269863 0.27924335 0.13548143 -0.2677968 0.03536313 0.3248672 0.2062973 0.29093853 0.1844036 -0.43359983 0.025519002 -0.06319317 -0.2427806 -0.22732906 0.08803728 -0.041860744 -0.151291 0.3400458 -0.29143015 0.25334117 0.06265491 0.26399022 -0.20121849 0.22156847 -0.50599706 0.069224015 0.52325517 -0.34115726 -0.105219565 -0.37346402 -0.02126528 0.09619415 0.017722093 -0.3621799 -0.109912336 0.021542747 -0.13361925 0.2087667 -0.08780184 0.09494446 -0.25047818 -0.07924239 0.21750642 0.2621652 -0.52888566 0.081884995 -0.20485449 0.18029206 -0.5623824 -0.03897387 0.3213515 0.057455678 -0.26524526 0.14741589 0.1257589 0.04708992 0.026751317 -0.014696863 -0.11038961 0.004459205 -0.01394376 0.091146186 -0.15486309 0.20662159 -0.0987916 -0.07740813 0.009704136 0.28866896 0.3916269 0.35061485 0.31678385 0.43233085 0.44510433\n",
    "\n",
    "For these vectors, I used the industry-standard of 300 dimensions. We see each of these dimensions represented by each of the floats, separated by whitespace. As the model passes over the corpus it is being trained on, it hones these numbers and changes them for each word. Over multiple epochs, or generations, it gains a clearer sense of the similarity of words, or at least words that are used in similar contexts.\n",
    "\n",
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. AI work primarily with Holocaust and human rights abuses documents. For this reason, I will use a word vector model that I have trained on Holocaust documents. Consider the word \"concentration camp\". Let’s now use these word vectors to find the 10 most similar words to concentration camp.\n",
    "\n",
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. At the start of the notebook, I asked you to consider the word concentration camp. Let’s now use these word vectors to find the 10 most similar words to concentration camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "isolated-updating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('extermination_camp', 0.5768706798553467),\n",
       " ('camp', 0.5369070172309875),\n",
       " ('Flossenbiirg', 0.5099129676818848),\n",
       " ('Sachsenhausen', 0.5068483948707581),\n",
       " ('Auschwitz', 0.48929861187934875),\n",
       " ('Dachau', 0.4765608310699463),\n",
       " ('concen', 0.4753464460372925),\n",
       " ('Majdanek', 0.4740387797355652),\n",
       " ('Sered', 0.47086501121520996),\n",
       " ('Buchenwald', 0.4692303538322449)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    ('extermination_camp', 0.5768706798553467),\n",
    "    ('camp', 0.5369070172309875),\n",
    "    ('Flossenbiirg', 0.5099129676818848),\n",
    "    ('Sachsenhausen', 0.5068483948707581),\n",
    "    ('Auschwitz', 0.48929861187934875),\n",
    "    ('Dachau', 0.4765608310699463),\n",
    "    ('concen', 0.4753464460372925),\n",
    "    ('Majdanek', 0.4740387797355652),\n",
    "    ('Sered', 0.47086501121520996),\n",
    "    ('Buchenwald', 0.4692303538322449)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-handy",
   "metadata": {},
   "source": [
    "These are the items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Extermination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to concentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: “concen”. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let’s do something similar with Auschwitz.items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Extermination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to concentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: “concen”. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concentration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let’s do something similar with Auschwitz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infinite-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Auschwitz_Birkenau', 0.6649479866027832),\n",
       " ('Birkenau', 0.5385118126869202),\n",
       " ('subcamp', 0.5343026518821716),\n",
       " ('camp', 0.533636748790741),\n",
       " ('III', 0.5323576927185059),\n",
       " ('stutthof', 0.518073320388794),\n",
       " ('Ravensbriick', 0.5084848403930664),\n",
       " ('Berlitzer', 0.5083401203155518),\n",
       " ('Malchow', 0.5051567554473877),\n",
       " ('Oswiecim', 0.5016494393348694)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    ('Auschwitz_Birkenau', 0.6649479866027832),\n",
    "    ('Birkenau', 0.5385118126869202),\n",
    "    ('subcamp', 0.5343026518821716),\n",
    "    ('camp', 0.533636748790741),\n",
    "    ('III', 0.5323576927185059),\n",
    "    ('stutthof', 0.518073320388794),\n",
    "    ('Ravensbriick', 0.5084848403930664),\n",
    "    ('Berlitzer', 0.5083401203155518),\n",
    "    ('Malchow', 0.5051567554473877),\n",
    "    ('Oswiecim', 0.5016494393348694)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-frank",
   "metadata": {},
   "source": [
    "As we can see, the words closest to Auchwitz are places assocaited with Auschwitz, such as Birkenau, subcamps (of which Auschwitz had many), other concentration camps (such as Ravensbriick), and the location of the Auschwitz memorial, Oswiecim.\n",
    "\n",
    "In other words, we have words closely associated with Auschwitz in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "surprised-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "random-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"data/alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "final-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(search_word, model, k=10):\n",
    "    res =  model.get_nearest_neighbors(search_word, k=k)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spiritual-causing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.999941885471344, 'know,'),\n",
       " (0.9999399781227112, \"know,'\"),\n",
       " (0.99993497133255, 'shouted'),\n",
       " (0.9999307990074158, 'grown'),\n",
       " (0.9999297857284546, 'crowded'),\n",
       " (0.9999275207519531, 'followed'),\n",
       " (0.9999272227287292, 'know'),\n",
       " (0.9999259114265442, 'meaning'),\n",
       " (0.9999238848686218, 'something'),\n",
       " (0.9999237060546875, \"know.'\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_matches(\"known\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "spatial-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_model = fasttext.train_unsupervised(\"data/100.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "animal-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9504625797271729, 'Carolus,'),\n",
       " (0.9000089168548584, 'Carolum'),\n",
       " (0.8991857767105103, 'Caroli'),\n",
       " (0.8934779763221741, 'Carolum,'),\n",
       " (0.8916164040565491, 'Carolo'),\n",
       " (0.8890672326087952, 'Carolo,'),\n",
       " (0.8861714601516724, 'Ludovicus'),\n",
       " (0.8721418380737305, 'imperator'),\n",
       " (0.8589209914207458, 'Aldricus'),\n",
       " (0.8531587719917297, 'Offa')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_matches(\"Carolus\", latin_model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-compensation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eligible-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relationships(base_word, is_to, second_word, model):\n",
    "    as_blank = model.get_analogies(base_word, is_to, second_word)\n",
    "    return (as_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "searching-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9006853699684143, 'Ferrariensis'),\n",
       " (0.900662899017334, 'Sigulfus'),\n",
       " (0.8969728350639343, 'Ferrariensi'),\n",
       " (0.8933336138725281, 'Lupi'),\n",
       " (0.8929973244667053, 'Parisiensis'),\n",
       " (0.8918678760528564, 'Sigulfo'),\n",
       " (0.8896274566650391, 'Turonense'),\n",
       " (0.8875166773796082, 'Turonensis'),\n",
       " (0.8857744932174683, 'Eborac.'),\n",
       " (0.8843535780906677, 'Fuldensi')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_relationships(\"Carolus\", \"rex\", \"abbas\", latin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-sperm",
   "metadata": {},
   "source": [
    "## Cultivating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-penny",
   "metadata": {},
   "source": [
    "I will be demonstrating this outside of the notebook with Prodigy because this is proprietary software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-liverpool",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "floral-murder",
   "metadata": {},
   "source": [
    "## Encoding Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-homework",
   "metadata": {},
   "source": [
    "Another issue under-resourced languages frequently face is something collectively known as encoding issues. To understand why this is a problem, you need to understand a bit about how text is represented in a compute.\n",
    "\n",
    "Text as we see it is not how a computer processes that information. A computer understands a character that represents a letter numerically. This number is understood as a combination of 0s and 1s which is how all computers with the exception of quantum computers process data. This is due to how computers are structured at the hardware level, that is, as a series of gates that are either on or off--0 or 1.\n",
    "\n",
    "Binary can represent complex things and through different types of gates it can do addition and subtraction and once you can do addition and subtraction, you can pretty much do anything. To prove this, think about multiplication. Computers do not multiply. They do not do 5 x 10 as single problem, rather than add 5 10 times.\n",
    "\n",
    "For numbers to be expressed in binary, it is fairly straight forward, you convert down from our base-10 system to a base-2 system. Imagine if you had a number: 14. We use two digits to express this numerical value. The first digit from the left is 4. This is the ones place that denotes 4 single items of something. The next digit is in the tens place and it denotes that 4 must be added to 1 x 10. This is how we use base 10 to arrive at 14.\n",
    "\n",
    "In binary, we do the same thing, but instead of a 1s place and a 10s place, you have the same concept truncated to 0 and 1.\n",
    "\n",
    "To express 14 in binary, we need 4 places. In binary 14 is 1110. Why do we need four places? Because 14 must be expressed in base-2. To understand this better, let's look at the number 2.\n",
    "\n",
    "2 in binary is 10.\n",
    "\n",
    "It is 0 in the initial position and 1 in the second. So, it is (0 x 0) + (1 x 2). This results in 2 in base-10.\n",
    "\n",
    "Let's return to 14 or 1110.\n",
    "\n",
    "Imagine that it is the same as writing (in reverse order) (0x1)+(1x2)+(1x4)+(1x8). We increase by multiples of 2s, rather than 10s because we are in base-2. Go ahead and try to do that base-10 math on your calculator. You get 0+2+4+8. If you add that together, you get 14.\n",
    "\n",
    "Here's the problem. We can easily represent numbers numerically in this fashion, but how do we represent characters like letters. The answer is a bit complicated. We need to represent it as numbers and we do that using bytes, or a combination of 8 bits.\n",
    "\n",
    "To represent the letter \"a\", for example, we can use the binary => 01100001. Numerically, this corresponds to the number 97. With 8 bytes, we can have a max value of 11111111 which equates to 256 different numbers.\n",
    "\n",
    "Early in the development of computers, these values were used to represent texts through what is known as ASCII, the American Standard Code for Information Exchange. To see how this worked, check out the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-portal",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/USASCII_code_chart.png/1280px-USASCII_code_chart.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-equality",
   "metadata": {},
   "source": [
    "While Ascii was revolutionary, it introduced problems, especially as those outside of America began to use computers for text purposes. In Europe, there were characters unrepresented in the American alphabet. These could be added in, but as more speakers began to use computers the ability the encode these characters through Ascii proved impossible. This was particularly true in Asia, especially Japan, where characters, not letters, needed to be represented.\n",
    "\n",
    "You can now see the mounting problem. Different countries invented their own encoding methods to suit their needs. And this worked until you needed to send a document from Japan to England or from England to Germany. The documents would not be decoded correctly and would come out illegible.\n",
    "\n",
    "Enter the internet of the 80s and and the mass internet boom of the 90s and you can now see the increasing problem.\n",
    "\n",
    "Who will save us from this disaster!\n",
    "\n",
    "Enter... Unicode or the Universal Coded Character.\n",
    "\n",
    "It is a universal standard that can account for 1,111,998 characters, everything from \"j\" to \"é\" to 😀. Unicode solved problems. But now a new problem exists for us working with ancient or medieval languages. They may be encoded in an earlier pre-Unicode encoding. Or they may be standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-trouble",
   "metadata": {},
   "source": [
    "These are largely two different problems. Let's look at normalization with the Python library unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dietary-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-native",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*_irZeGalg3lwm2pQoHtUFw.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "elect-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar = \"Galliā est omnīs dīvīsa in partēs trēs, quārum ūnam incolunt Belgae, aliam Aquītānī, tertiam quī ipsōrum linguā Celtae, nostra Gallī appellantur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sorted-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galliā est omnīs dīvīsa in partēs trēs, quārum ūnam incolunt Belgae, aliam Aquītānī, tertiam quī ipsōrum linguā Celtae, nostra Gallī appellantur.\n"
     ]
    }
   ],
   "source": [
    "print (caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "grateful-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_caesar = normalize(u'NFKD', caesar).encode('ascii', 'ignore').decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "respiratory-tolerance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\n"
     ]
    }
   ],
   "source": [
    "print (decoded_caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-south",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
